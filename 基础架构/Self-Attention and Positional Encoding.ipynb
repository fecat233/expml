{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd3288cc-6243-40f4-88c8-55c45ddd3173",
   "metadata": {},
   "source": [
    "# Self-Attention and Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f958fa7a-6021-4d08-9426-15dae775a4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49851df-544b-4736-9ff4-0a56f8a0ab22",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b241647-9513-45ce-9e47-bfd361b0f853",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim, input_dim=None, proj_values=False):\n",
    "        super(Attention, self).__init__()\n",
    "        self.d_k = hidden_dim\n",
    "        self.input_dim = hidden_dim if input_dim is None else input_dim\n",
    "        self.proj_values = proj_values\n",
    "        # Affine transformation for q, k , v\n",
    "        self.linear_query = nn.Linear(self.input_dim, hidden_dim)\n",
    "        self.linear_key = nn.Linear(self.input_dim, hidden_dim)\n",
    "        self.linear_value = nn.Linear(self.input_dim, hidden_dim)\n",
    "        self.alphas = None\n",
    "    \n",
    "    def init_keys(self, keys):\n",
    "        self.keys = keys\n",
    "        self.proj_keys = self.linear_key(self.keys)\n",
    "        self.values = self.linear_value(self.keys) if self.proj_values else self.keys\n",
    "        \n",
    "    # alignment scores\n",
    "    def score_function(self, query):\n",
    "        proj_query = self.linear_query(query)\n",
    "        dot_products = torch.bmm(proj_query, self.proj_keys.permute(0, 2, 1))\n",
    "        scores = dot_products / np.sqrt(self.d_k)\n",
    "        return scores\n",
    "    \n",
    "    def forward(self, query, mask=None):\n",
    "        scores = self.score_function(query)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        alphas = F.softmax(scores, dim=-1)\n",
    "        self.alphas = alphas.detach()\n",
    "        \n",
    "        context = torch.bmm(alphas, self.values)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69901f15-3880-45c4-b03f-24226b964b8c",
   "metadata": {},
   "source": [
    "## Muti-Headed Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd709ff2-6410-421f-a179-a73e0a0cb605",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, input_dim=None, proj_values=True):\n",
    "        super(MutiHeadAttention, self).__init__()\n",
    "        self.linear_out = nn.Linear(n_heads * d_model, d_model)\n",
    "        self.attn_heads = nn.ModuleList(\n",
    "            [Attention(d_model, input_dim=input_dim, proj_values=proj_values) for _ in range(n_heads)]\n",
    "        )\n",
    "    \n",
    "    def init_keys(self, key):\n",
    "        for attn in self.attn_heads:\n",
    "            attn.init_keys(key)\n",
    "    \n",
    "    @property\n",
    "    def alphas(self):\n",
    "        return torch.stack(\n",
    "            [attn.alphas for attn in self.attn_heads], dim=0\n",
    "        )\n",
    "    \n",
    "    def output_function(self, contexts):\n",
    "        concatenated = torch.cat(contexts, axis=-1)\n",
    "        out = self.linear_out(concatenated)\n",
    "        return out\n",
    "    \n",
    "    def forward(self, query, mask=None):\n",
    "        contexts = [attn(query, mask=mask) for attn in self.attn_heads]\n",
    "        out = self.output_function(contexts)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab9c403-53cf-45bf-b5f9-eeb5c5dec5e7",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9be1c134-5497-4868-88ba-52537463a0fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1., -1.],\n",
       "         [-1.,  1.],\n",
       "         [ 1.,  1.],\n",
       "         [ 1., -1.]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_seq = torch.tensor([[-1, -1], [-1, 1], [1, 1], [1, -1]]).float().view(1, 4, 2)\n",
    "full_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6c885da-2240-48be-a73a-0823c2fb3699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1., -1.],\n",
       "          [-1.,  1.]]]),\n",
       " tensor([[[ 1.,  1.],\n",
       "          [ 1., -1.]]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_seq = full_seq[:, :2, :]\n",
    "target_seq = full_seq[:, 2:, :]\n",
    "source_seq, target_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abeabedf-cf51-4420-9111-ef03ad46a41b",
   "metadata": {},
   "source": [
    "## Encoder + Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf2414f5-3a53-4a2f-900b-1fbfb6222521",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, ff_units, n_features=None):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.ff_units = ff_units\n",
    "        self.n_features = n_features\n",
    "        self.self_attn_heads = MutiHeadAttention(n_heads, d_model, input_dim=n_features)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, ff_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_units, d_model),\n",
    "        )\n",
    "    \n",
    "    def forward(self, query, mask=None):\n",
    "        self.self_attn_heads.init_keys(query)\n",
    "        att = self.self_attn_heads(query, mask)\n",
    "        out = self.ffn(att)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d344575f-8e4b-4513-88c1-634a8c52ab6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0498,  0.2193],\n",
       "         [-0.0642,  0.2258]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(11)\n",
    "encoder = Encoder(n_heads=3, d_model=2, ff_units=10, n_features=2)\n",
    "query = source_seq\n",
    "encoder_states = encoder(query)\n",
    "encoder_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc136cb3-7ce5-455e-8575-14678bd17fae",
   "metadata": {},
   "source": [
    "## Decoder + Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d9977ad-cebc-4dc4-8241-cd381ed0e0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, ff_units, n_features=None):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.ff_units = ff_units\n",
    "        self.n_features = d_model if n_features is None else n_features\n",
    "        self.self_attn_heads = MutiHeadAttention(n_heads, d_model, input_dim=self.n_features)\n",
    "        self.cross_attn_heads = MutiHeadAttention(n_heads, d_model)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, ff_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_units, self.n_features)\n",
    "        )\n",
    "        \n",
    "    def init_keys(self, states):\n",
    "        self.cross_attn_heads.init_keys(states)\n",
    "        \n",
    "    def forward(self, query, source_mask=None, target_mask=None):\n",
    "        self.self_attn_heads.init_keys(query)\n",
    "        att1 = self.self_attn_heads(query, target_mask)\n",
    "        att2 = self.cross_attn_heads(att1, source_mask)\n",
    "        out = self.ffn(att2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa39b670-1b1d-4db1-933a-948b193823d1",
   "metadata": {},
   "source": [
    "## target mask(traning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb7d0a87-4f42-43ea-897b-ebd5dac65d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.,  1.],\n",
       "         [ 1.,  1.]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shifted_seq = torch.cat([source_seq[:, -1:], target_seq[:, :-1]], dim=1)\n",
    "shifted_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "000c0123-5ba8-4f0f-a506-a601389c7068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = (1 - torch.triu(torch.ones(attn_shape), diagonal=1)).bool()\n",
    "    return subsequent_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59acc25a-85d2-4caf-a26d-d51e418373b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False],\n",
       "         [ True,  True]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsequent_mask(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c892dd65-fdb8-4f19-bc6e-74b6a6e92604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.0000, 0.0000],\n",
       "          [0.4011, 0.5989]]],\n",
       "\n",
       "\n",
       "        [[[1.0000, 0.0000],\n",
       "          [0.4264, 0.5736]]],\n",
       "\n",
       "\n",
       "        [[[1.0000, 0.0000],\n",
       "          [0.6304, 0.3696]]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(13)\n",
    "decoder = Decoder(n_heads=3, d_model=2, ff_units=10, n_features=2)\n",
    "decoder.init_keys(encoder_states)\n",
    "\n",
    "query = shifted_seq\n",
    "out = decoder(query, target_mask=subsequent_mask(2))\n",
    "\n",
    "decoder.self_attn_heads.alphas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadbdf4f-4df2-4ccd-84bc-587a174808d2",
   "metadata": {},
   "source": [
    "## target mask(evaluation/prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57af3d0d-3e37-41b9-8ea7-35c1135688dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4132, 0.3728]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = source_seq[:, -1:]\n",
    "trg_masks = subsequent_mask(1)\n",
    "out = decoder(inputs, trg_masks)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22b2e7b7-9ada-4e6c-9496-6994f8c32a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0000,  1.0000],\n",
       "         [ 0.4132,  0.3728]]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.cat([inputs, out[:, -1:, :]], dim=-2)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f1f2921-d345-4bae-bd0f-912acf76ff73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4137, 0.3727],\n",
       "         [0.4132, 0.3728]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_masks = subsequent_mask(2)\n",
    "out = decoder(inputs, trg_masks)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3b175c-ee48-4107-bfe1-b548fc6bb729",
   "metadata": {},
   "source": [
    "## Encoder + Decoder + Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e02983c-2d6a-478e-a0fa-51df76be2695",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, input_len, target_len):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.input_len = input_len\n",
    "        self.target_len = target_len\n",
    "        self.trg_mask = self.subsequent_mask(self.target_len)\n",
    "    \n",
    "    @staticmethod\n",
    "    def subsequent_mask(size):\n",
    "        attn_shape = (1, size, size)\n",
    "        subsequent_mask = (1 - torch.triu(torch.ones(attn_shape), diagonal=1)).bool()\n",
    "        return subsequent_mask\n",
    "    \n",
    "    def encode(self, source_seq, source_mask):\n",
    "        # Encodes the source sequence and uses the result\n",
    "        # to initialize the decoder\n",
    "        encoder_states = self.encoder(source_seq, source_mask)\n",
    "        self.decoder.init_keys(encoder_states)\n",
    "        \n",
    "    def decode(self, shifted_target_seq, source_mask=None, target_mask=None):\n",
    "        # Decodes/generates a sequence using the shifted (masked) target sequence - used in TRAIN mode\n",
    "        outputs = self.decoder(shifted_target_seq, source_mask=source_mask, target_mask=target_mask)\n",
    "        return outputs\n",
    "    \n",
    "    def predict(self, source_seq, source_mask):\n",
    "        # Decodes/generates a sequence using one input at a time - used in EVAL mode\n",
    "        inputs = source_seq[:, -1:]\n",
    "        for i in range(self.target_len):\n",
    "            out = self.decode(inputs, source_mask, self.trg_mask[:, :i+1, :i+1])\n",
    "            out = torch.cat([inputs, out[:, -1:, :]], dim=-2)\n",
    "            inputs = out.detach()\n",
    "        outputs = inputs[:, 1:, :]\n",
    "        return outputs\n",
    "    \n",
    "    def forward(self, X, source_mask=None):\n",
    "        # Sends the mask to the same device as the inputs\n",
    "        self.trg_mask = self.trg_mask.type_as(X).bool()\n",
    "        source_seq = X[:, :self.input_len, :]\n",
    "        self.encode(source_seq, source_mask)\n",
    "        if self.traning:\n",
    "            shifted_target_seq = X[:, self.input_len-1:-1, :]\n",
    "            outputs = self.decode(shifted_target_seq, source_mask, self.trg_mask)\n",
    "        else:\n",
    "            outputs = self.predict(source_seq, source_mask)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9d8a2a-1d44-44e3-839b-b758aec5e931",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78c10a99-12e4-4918-a178-19facf921945",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_len, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        angular_speed = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * angular_speed) # even dimensions\n",
    "        pe[:, 1::2] = torch.cos(position * angular_speed) # odd dimensions\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        scaled_x = x * np.sqrt(self.d_model)\n",
    "        encoded = scaled_x + self.pe[:, :x.size(1), :]\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ac1b625-265d-406e-9e29-a5bd34d2db1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "posenc = PositionalEncoding(2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23d4e381-7092-437f-8601-c4f711832476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([],\n",
       " OrderedDict([('pe', tensor([[[0.0000, 1.0000],\n",
       "                        [0.8415, 0.5403]]]))]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(posenc.parameters()), posenc.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c11b9e86-b9f0-443a-b547-3a10686b5deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1., -1.],\n",
       "         [-1.,  1.]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8c1fc72-93bf-4d5d-a16d-46190ff48448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0000,  0.0000],\n",
       "         [-0.1585,  1.5403]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_seq + posenc.pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e6afef1-8a20-4729-b028-303754a7c8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderPe(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, ff_units, n_features=None, max_len=100):\n",
    "        super(Encoder, self).__init__()\n",
    "        pe_dim = d_model if n_features is None else n_features\n",
    "        self.pe = PositionalEncoding(max_len, pe_dim)\n",
    "        self.layer = Encoder(n_heads, d_model, ff_units, n_features)\n",
    "        \n",
    "    def forward(self, query, mask=None):\n",
    "        query_pe = self.pe(query)\n",
    "        out = self.layer(query_pe, mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd5830d6-f081-4115-82af-02070463a86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderPe(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, ff_units, n_features=None, max_len=100):\n",
    "        super(DecoderPe, self).__init__()\n",
    "        pe_dim = d_model if n_features is None else n_features\n",
    "        self.pe = PositionalEncoding(max_len, pe_dim)\n",
    "        self.layer = Decoder(n_heads, d_model, ff_untis, n_features)\n",
    "        \n",
    "    def init_keys(self, states):\n",
    "        self.layer.init_keys(states)\n",
    "    \n",
    "    def forward(self, query, source_mask=None, target_mask=None):\n",
    "        query_pe = self.pe(query)\n",
    "        out = self.layer(query_pe, source_mask, target_mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79beb872-99e9-4797-b90f-fa86efd0f213",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
