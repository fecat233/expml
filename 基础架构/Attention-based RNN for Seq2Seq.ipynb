{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7272fc62-9f70-45e1-a574-2ec7ce142619",
   "metadata": {},
   "source": [
    "# Attention-based RNN for Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "943105e9-cd2c-4fb6-ae1b-b6e933c2d560",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae15c0d-fcb6-4ef3-a3ff-c5c6f7b47a7f",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12dac7c9-1789-4659-b2b1-2cb0f62ff793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_features, num_layers=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_features = n_features\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden = None\n",
    "        self.rnn = nn.GRU(self.n_features, self.hidden_dim, self.num_layers, batch_first=True)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        output, self.hidden = self.rnn(X)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7305199b-b57e-4863-9aec-29f482c1af0a",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4db15c0-8475-457a-ad01-b9a1a50845c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_features, num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_features = n_features\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden = None\n",
    "        self.rnn = nn.GRU(self.n_features, self.hidden_dim, self.num_layers, batch_first=True)\n",
    "        self.regression = nn.Linear(self.hidden_dim, self.n_features)\n",
    "        \n",
    "    def init_hidden(self, hidden_seq):\n",
    "        hidden_final = hidden_seq[:, -1:]\n",
    "        self.hidden = hidden_final.permute(1, 0, 2)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        output, self.hidden = self.rnn(X, self.hidden)\n",
    "        last_output = output[:, -1:]\n",
    "        out = self.regression(last_output)\n",
    "        \n",
    "        return out.view(-1, 1, self.n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743b9c89-cea3-4c4a-b10c-af7873352bb1",
   "metadata": {},
   "source": [
    "## EncoderDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1840b3f9-9c44-4da1-9226-06904b275bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, input_len, target_len, teacher_forcing_prob):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.input_len = input_len\n",
    "        self.target_len = target_len\n",
    "        self.teacher_forcing_prob = teacher_forcing_prob\n",
    "        self.outputs = None\n",
    "    \n",
    "    def init_outputs(self, batch_size):\n",
    "        self.outputs = torch.zeros(batch_size, self.target_len, self.encoder.n_features)\n",
    "    \n",
    "    def store_output(self, i, out):\n",
    "        self.outputs[:, i:i+1, :] = out\n",
    "        \n",
    "    def forward(self, X):\n",
    "        source_seq = X[:, :self.input_len, :]\n",
    "        target_seq = X[:, self.input_len:, :]\n",
    "        self.init_outputs(X.shape[0])\n",
    "        \n",
    "        hidden_seq = self.encoder(source_seq)\n",
    "        self.decoder.init_hidden(hidden_seq)\n",
    "        \n",
    "        des_inputs = source_seq[:, -1:]\n",
    "        \n",
    "        for i in range(self.target_len):\n",
    "            out = self.decoder(des_inputs)\n",
    "            self.store_output(i, out)\n",
    "            \n",
    "            prob = self.teacher_forcing_prob\n",
    "            \n",
    "            if not self.training:\n",
    "                prob = 0\n",
    "            \n",
    "            if torch.rand(1) <= prob:\n",
    "                des_inputs = target_seq[:, i:i+1, :]\n",
    "            else:\n",
    "                des_inputs = out\n",
    "        return self.outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cca88a3-7142-433c-a36d-d4a1fdea0e29",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32dcea24-49a5-4779-a7a1-913365da06df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1., -1.],\n",
       "         [-1.,  1.],\n",
       "         [ 1.,  1.],\n",
       "         [ 1., -1.]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_seq = torch.tensor([[-1, -1], [-1, 1], [1, 1], [1, -1]]).float().view(1, 4, 2)\n",
    "full_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d23f9b72-5035-4987-bf89-092a79e3165a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1., -1.],\n",
       "          [-1.,  1.]]]),\n",
       " tensor([[[ 1.,  1.],\n",
       "          [ 1., -1.]]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_seq = full_seq[:, :2, :]\n",
    "target_seq = full_seq[:, 2:, :]\n",
    "source_seq, target_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f81b74b-01f4-45d1-b7e4-8d439da08a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0832, -0.0356],\n",
       "         [ 0.3105, -0.5263]]], grad_fn=<TransposeBackward1>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(21)\n",
    "encoder = Encoder(n_features=2, hidden_dim=2)\n",
    "hidden_seq = encoder(source_seq)\n",
    "values = hidden_seq\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d663923-c518-4ef6-933b-d57183a02ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3105, -0.5263]]], grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ee0adb5-b993-4e25-8382-3a63a820bc61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0832, -0.0356],\n",
       "         [ 0.3105, -0.5263]]], grad_fn=<TransposeBackward1>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = hidden_seq\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e56d7af-2395-40e5-a7b2-64f4990b7603",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(21)\n",
    "decoder = Decoder(n_features=2, hidden_dim=2)\n",
    "decoder.init_hidden(hidden_seq)\n",
    "\n",
    "inputs = source_seq[:, -1:, :]\n",
    "out = decoder(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2140fe2d-e4da-4ec2-ae1a-f58690efc996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3913, -0.6853]]], grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3408cdc-0862-45f3-8398-33a3fda2554c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3913, -0.6853]]], grad_fn=<PermuteBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = decoder.hidden.permute(1, 0, 2)\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bc50f1f-e59f-4603-9a7a-cd03b7aa7d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5000, 0.5000]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_alphas(ks, q):\n",
    "    N, L, H = ks.size()\n",
    "    alphas = torch.ones(N, 1, L).float() * 1/L\n",
    "    return alphas\n",
    "\n",
    "alphas = calc_alphas(keys, query)\n",
    "alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb97af42-1b55-4ef3-a899-97999ee97b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1968, -0.2809]]], grad_fn=<BmmBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector = torch.bmm(alphas, values)\n",
    "context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f12bccd-d107-4827-9b1a-d1fd2eeec539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1968, -0.2809,  0.3913, -0.6853]]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated = torch.cat([context_vector, query], axis=-1)\n",
    "concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72847cff-4ff8-4b44-8c98-791d093f8897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0569, 0.4821]]], grad_fn=<BmmBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products = torch.bmm(query, keys.permute(0, 2, 1)) #alignment scores\n",
    "products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "069357f9-1839-4ff7-8a28-9d41d038ed63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3953, 0.6047]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphas = F.softmax(products, dim=-1) # attention scores\n",
    "alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38811206-4f0c-4165-a0e6-84dfa2e42947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_alphas(ks, q):\n",
    "    products = torch.bmm(q, ks.permute(0, 2, 1))\n",
    "    alphas = F.softmax(products, dim=-1)\n",
    "    return alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f170e0a-5cd2-40b3-80c5-2d0631725081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0403, 0.3409]]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = query.size(-1)\n",
    "scaled_products = products / np.sqrt(dim)\n",
    "scaled_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ff84071-eacd-4e41-b598-a6d9216baa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_alphas(ks, q):\n",
    "    dims = q.size(-1)\n",
    "    products = torch.bmm(q, ks.permute(0, 2, 1))\n",
    "    scaled_products = products / np.sqrt(dim)\n",
    "    alphas = F.softmax(scaled_products, dim=-1)\n",
    "    return alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f8b8352-ba89-4cec-a2e9-6f7b7a29b5f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2138, -0.3175]]], grad_fn=<BmmBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphas = calc_alphas(keys, query)\n",
    "context_vector = torch.bmm(alphas, values)\n",
    "context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bf85d8e4-b81a-49e2-9d2e-26e151d4d887",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim, input_dim=None, proj_values=False):\n",
    "        super(Attention, self).__init__()\n",
    "        self.d_k = hidden_dim\n",
    "        self.input_dim = hidden_dim if input_dim is None else input_dim\n",
    "        self.proj_values = proj_values\n",
    "        # Affine transformation for q, k , v\n",
    "        self.linear_query = nn.Linear(self.input_dim, hidden_dim)\n",
    "        self.linear_key = nn.Linear(self.input_dim, hidden_dim)\n",
    "        self.linear_value = nn.Linear(self.input_dim, hidden_dim)\n",
    "        self.alphas = None\n",
    "    \n",
    "    def init_keys(self, keys):\n",
    "        self.keys = keys\n",
    "        self.proj_keys = self.linear_key(self.keys)\n",
    "        self.values = self.linear_value(self.keys) if self.proj_values else self.keys\n",
    "        \n",
    "    # alignment scores\n",
    "    def score_function(self, query):\n",
    "        proj_query = self.linear_query(query)\n",
    "        dot_products = torch.bmm(proj_query, self.proj_keys.permute(0, 2, 1))\n",
    "        scores = dot_products / np.sqrt(self.d_k)\n",
    "        return scores\n",
    "    \n",
    "    def forward(self, query, mask=None):\n",
    "        scores = self.score_function(query)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        alphas = F.softmax(scores, dim=-1)\n",
    "        self.alphas = alphas.detach()\n",
    "        \n",
    "        context = torch.bmm(alphas, self.values)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a08aa366-092a-4de1-821d-3a502f8ee189",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_seq = torch.tensor([[[-1., 1.], [0., 0.]]])\n",
    "# pretend there's an encoder here...\n",
    "keys = torch.tensor([[[-.38, .44], [.85, -.05]]])\n",
    "query = torch.tensor([[[-1., 1.]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a0af24a-5da0-4304-92d0-03848f37d237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_mask = (source_seq != 0).all(axis=2).unsqueeze(1)\n",
    "source_mask # N, 1, L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b104b54-7ad8-4649-9229-57bace0325d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0.]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(11)\n",
    "attnh = Attention(2)\n",
    "attnh.init_keys(keys)\n",
    "\n",
    "context = attnh(query, mask=source_mask)\n",
    "attnh.alphas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7e19c2-b62c-45b4-a7db-999f9a5beb90",
   "metadata": {},
   "source": [
    "## Decoder + Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5e4d54f8-4513-4285-9c95-2ba6afff6d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderAttn(nn.Module):\n",
    "    def __init__(self, n_features, hidden_dim):\n",
    "        super(DecoderAttn, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_features = n_features\n",
    "        self.hidden = None\n",
    "        self.rnn = nn.GRU(self.n_features, self.hidden_dim, batch_first=True)\n",
    "        self.attn = Attention(self.hidden_dim)\n",
    "        self.regression = nn.Linear(2 * self.hidden_dim, self.n_features)\n",
    "    \n",
    "    def init_hidden(self, hidden_seq):\n",
    "        self.attn.init_keys(hidden_seq)\n",
    "        hidden_final = hidden_seq[:, -1:]\n",
    "        self.hidden = hidden_final.permute(1, 0, 2)\n",
    "        \n",
    "    def forward(self, X, mask=None):\n",
    "        output, self.hidden = self.rnn(X, self.hidden)\n",
    "        query = output[:, -1:]\n",
    "        context = self.attn(query, mask=mask)\n",
    "        concatenated = torch.cat([context, query], axis=-1)\n",
    "        out = self.regression(concatenated)\n",
    "        \n",
    "        return out.view(-1, 1, self.n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b200d6ed-7ace-41bf-86ec-7961b6d81c89",
   "metadata": {},
   "source": [
    "## Encoder + Decoder + Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80bc3f65-7cdc-4512-99af-4092bfed3950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1., -1.],\n",
       "         [-1.,  1.],\n",
       "         [ 1.,  1.],\n",
       "         [ 1., -1.]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_seq = torch.tensor([[-1, -1], [-1, 1], [1, 1], [1, -1]]).float().view(1, 4, 2)\n",
    "full_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e1108ce5-55e4-4d5d-8bba-17ce2b1187ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1., -1.],\n",
       "          [-1.,  1.]]]),\n",
       " tensor([[[ 1.,  1.],\n",
       "          [ 1., -1.]]]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_seq = full_seq[:, :2, :]\n",
    "target_seq = full_seq[:, 2:, :]\n",
    "source_seq, target_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fe386f8b-29cb-427f-9f21-74546efdd3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor([[[-0.3555, -0.1220]]], grad_fn=<ViewBackward0>)\n",
      "Output tensor([[[-0.2641, -0.2521]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(21)\n",
    "encoder = Encoder(n_features=2, hidden_dim=2)\n",
    "decoder_attn = DecoderAttn(n_features=2, hidden_dim=2)\n",
    "\n",
    "hidden_seq = encoder(source_seq)\n",
    "decoder_attn.init_hidden(hidden_seq)\n",
    "\n",
    "input = source_seq[:, -1:]\n",
    "target_len = 2\n",
    "for i in range(target_len):\n",
    "    out = decoder_attn(inputs)\n",
    "    print(f'Output {out}')\n",
    "    inputs = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d318379a-9969-4318-8c54-a11decf178aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3555, -0.1220],\n",
       "         [-0.2641, -0.2521]]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encdec = EncoderDecoder(encoder, decoder_attn, input_len=2, target_len=2, teacher_forcing_prob=0.0)\n",
    "encdec(full_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b315bf8-eb5e-419c-9b7e-a3aff8ab05ed",
   "metadata": {},
   "source": [
    "## Muti-Headed Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9cafb166-ae6e-4caf-8864-4c5d42614d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, input_dim=None, proj_values=True):\n",
    "        super(MutiHeadAttention, self).__init__()\n",
    "        self.linear_out = nn.Linear(n_heads * d_model, d_model)\n",
    "        self.attn_heads = nn.ModuleList(\n",
    "            [Attention(d_model, input_dim=input_dim, proj_values=proj_values) for _ in range(n_heads)]\n",
    "        )\n",
    "    \n",
    "    def init_keys(self, key):\n",
    "        for attn in self.attn_heads:\n",
    "            attn.init_keys(key)\n",
    "    \n",
    "    @property\n",
    "    def alphas(self):\n",
    "        return torch.stack(\n",
    "            [attn.alphas for attn in self.attn_heads], dim=0\n",
    "        )\n",
    "    \n",
    "    def output_function(self, contexts):\n",
    "        concatenated = torch.cat(contexts, axis=-1)\n",
    "        out = self.linear_out(concatenated)\n",
    "        return out\n",
    "    \n",
    "    def forward(self, query, mask=None):\n",
    "        contexts = [attn(query, mask=mask) for attn in self.attn_heads]\n",
    "        out = self.output_function(contexts)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e31401-19dd-4309-89d2-3becf946ab2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
